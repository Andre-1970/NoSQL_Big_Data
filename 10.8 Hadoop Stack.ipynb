{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjmhmcKYsQCD"
      },
      "source": [
        "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrjhqEZt5yme"
      },
      "source": [
        "# Стек Hadoop. Практическая работа\n",
        "\n",
        "## Цель практической работы\n",
        "\n",
        "Научиться использовать Hadoop MapReduce на практике."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VK7pV8G526W"
      },
      "source": [
        "## Что входит в работу\n",
        "\n",
        "* Загрузка данных в HDFS.\n",
        "* Получение данных из HDFS.\n",
        "* Реализация парадигмы MapReduce с применением Hadoop Streaming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VmvYym58Od"
      },
      "source": [
        "## Формат сдачи\n",
        "\n",
        "Отправьте в форме сдачи следующие файлы:\n",
        "- файл с результатом result.json;\n",
        "- ноутбук с кодом (все команды и функции, которые использовались для решения задач)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8d78B09sQCG"
      },
      "source": [
        "# Практическое задание\n",
        "\n",
        "Будем использовать логи сессий прослушивания музыкальных исполнителей в сервисе Spotify, сокращённую версию.\n",
        "\n",
        "https://www.aicrowd.com/challenges/spotify-sequential-skip-prediction-challenge/dataset_files\n",
        "\n",
        "Файл `spotify/log_mini.csv` содержит записи вида `ID сессии, номер в сессии, длинна сессии, id трека, skip_1, skip_2, ...`:\n",
        "```csv\n",
        "session_id,session_position,session_length,track_id_clean,skip_1,skip_2,skip_3,not_skipped,context_switch,no_pause_before_play,short_pause_before_play,long_pause_before_play,hist_user_behavior_n_seekfwd,hist_user_behavior_n_seekback,hist_user_behavior_is_shuffle,hour_of_day,date,premium,context_type,hist_user_behavior_reason_start,hist_user_behavior_reason_end\n",
        "0_00006f66-33e5-4de7-a324-2d18e439fc1e,1,20,t_0479f24c-27d2-46d6-a00c-7ec928f2b539,false,false,false,true,0,0,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
        "0_00006f66-33e5-4de7-a324-2d18e439fc1e,2,20,t_9099cd7b-c238-47b7-9381-f23f2c1d1043,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
        "```\n",
        "\n",
        "Вам нужно:\n",
        "1. **Посчитать для каждого трека количество его прослушиваний. Выведите два самых прослушиваемых трека.**\n",
        "2. **Вывести долю популярных треков: тех, что имеют больше 100 прослушиваний.**\n",
        "\n",
        "Для решения задачи:\n",
        "1. Скопируйте файлы в HDFS.\n",
        "2. Реализуйте подсчёт прослушиваний отдельным MapReduce, в файлы результата сохраните пары <track_id, listen_count>.\n",
        "3. С помощью команды `hdfs dfs -cat <YOUR-MAPRED-RESULT/*> | python stream_processor.py` решите три подзадачи:\n",
        "    1. Подсчитайте количество уникальных треков.\n",
        "    2. Посчитайте количество треков с количеством прослушиваний больше 20.\n",
        "    3. Найдите два самых популярных по listen_count.\n",
        "    \n",
        "    `stream_processor.py` — скрипт, читающий с потока ввода, необходимо реализовать самостоятельно.\n",
        "4. Сохраните результат работы скрипта выше в файл `result.json`, формат описан ниже.\n",
        "\n",
        "Реализуйте решение с использованием Hadoop MapReduce Streaming, для написания mapper и reducer используйте Python.\n",
        "\n",
        "Решение сохраните в локальный файл `result.json`, где по ключу q1\n",
        " запишите ответ на первый вопрос, по ключу q2 — на второй и по ключу q3 — на третий.\n",
        "\n",
        "\n",
        "## Критерии проверки\n",
        "\n",
        "1. Корректно реализован алгоритм подсчёта прослушиваний — mapper.py, reducer.py (без сохранения всех данных в память, работа с потоком).\n",
        "2. mapper.py и reducer.py протестированы локально.\n",
        "3. Данные ( `spotify/log_mini.csv` ) загружены в HDFS.\n",
        "4. Корректно запущен процесс Hadoop MapReduce Streaming с использованием mapper.py и reducer.py на данных.\n",
        "5. Корректно реализован `stream_processor.py` (без сохранения всех данных в память, работа с потоком).\n",
        "6. Результат записан в файл `result.json` и совпадает с эталонным.\n",
        "\n",
        "Пример содержимого файла `result.json`:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"q1\": [\"id1\", \"id2\"],\n",
        "    \"q2\": 0.13\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подключаемся к удаленной vm по ssh командой - ssh -i data/sshyandex admin@158.160.153.0\n",
        "# Запускаем Docker, выполняя команду - docker run -d -p 80:80 -p 8888:8888 sindq/sb-m8:latest\n",
        "# Получаем положительный результат на 5й раз ((( и грустим от потери средств - зис ис Yandex(\n",
        "\n",
        "\n",
        "\n",
        "# (.venv) ➜  NoSQL_Big_Data git:(main) ✗ ssh -i data/sshyandex admin@158.160.153.0\n",
        "# The authenticity of host '158.160.153.0 (158.160.153.0)' can't be established.\n",
        "# ED25519 key fingerprint is SHA256:sdfdf/dsfdsfsdfsdf+dfsdfsdf(убрал ключ).\n",
        "# This key is not known by any other names.\n",
        "# Are you sure you want to continue connecting (yes/no/[fingerprint])? yes\n",
        "# Warning: Permanently added '158.160.153.0' (ED25519) to the list of known hosts.\n",
        "# Enter passphrase for key 'data/sshyandex': \n",
        "# Welcome to Ubuntu 22.04.4 LTS (GNU/Linux 5.15.0-100-generic x86_64)\n",
        "\n",
        "#  * Documentation:  https://help.ubuntu.com\n",
        "#  * Management:     https://landscape.canonical.com\n",
        "#  * Support:        https://ubuntu.com/pro\n",
        "\n",
        "#   System information as of Tue Mar 12 01:13:00 AM UTC 2024\n",
        "\n",
        "#   System load:           1.89599609375\n",
        "#   Usage of /:            28.8% of 14.68GB\n",
        "#   Memory usage:          6%\n",
        "#   Swap usage:            0%\n",
        "#   Processes:             137\n",
        "#   Users logged in:       0\n",
        "#   IPv4 address for eth0: 172.18.0.12\n",
        "#   IPv6 address for eth0: 2a02:6b8:c03:500:0:f82e:9e64:20a\n",
        "\n",
        "\n",
        "# Expanded Security Maintenance for Applications is not enabled.\n",
        "\n",
        "# 0 updates can be applied immediately.\n",
        "\n",
        "# Enable ESM Apps to receive additional future security updates.\n",
        "# See https://ubuntu.com/esm or run: sudo pro status\n",
        "\n",
        "\n",
        "# The list of available updates is more than a week old.\n",
        "# To check for new updates run: sudo apt update\n",
        "\n",
        "\n",
        "# The programs included with the Ubuntu system are free software;\n",
        "# the exact distribution terms for each program are described in the\n",
        "# individual files in /usr/share/doc/*/copyright.\n",
        "\n",
        "# Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\n",
        "# applicable law.\n",
        "\n",
        "# To run a command as administrator (user \"root\"), use \"sudo <command>\".\n",
        "# See \"man sudo_root\" for details.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# admin@hadoopvm:~$ docker run -d -p 80:80 -p 8888:8888 sindq/sb-m8:latest\n",
        "# Unable to find image 'sindq/sb-m8:latest' locally\n",
        "# latest: Pulling from sindq/sb-m8\n",
        "# a48c500ed24e: Already exists \n",
        "# 1e1de00ff7e1: Already exists \n",
        "# 0330ca45a200: Already exists \n",
        "# 471db38bcfbf: Already exists \n",
        "# 0b4aba487617: Already exists \n",
        "# d44ea0cd796c: Already exists \n",
        "# 5ac827d588be: Already exists \n",
        "# d8d7747a335e: Already exists \n",
        "# 08790511e3e9: Already exists \n",
        "# e3c68aea9a5f: Already exists \n",
        "# 484c6d5fc38a: Already exists \n",
        "# 0448c1360cb9: Already exists \n",
        "# eed3d219811a: Extracting [=============>                                     ]  28.41MB/101.5MB\n",
        "# 944dbdf0ba87: Download complete \n",
        "# a881c93edb03: Download complete \n",
        "# bd3b8240d415: Download complete \n",
        "# 8991111fa910: Download complete \n",
        "# 8dc01f4de9c9: Download complete \n",
        "# aba43354736a: Download complete \n",
        "# 74d185ef112b: Download complete \n",
        "# baa81842b3da: Download complete \n",
        "# dd956fb4b809: Download complete \n",
        "# 76625c9b2859: Download complete \n",
        "# 6e376106829c: Download complete \n",
        "# f4eaabcceef7: Download complete \n",
        "# 7521e428e11d: Download complete \n",
        "# 498d95945650: Downloading [=========================>                         ]  351.4MB/697.9MB\n",
        "# 326e6472b212: Downloading [======================>                            ]  316.1MB/697.9MB\n",
        "# 1d905699d48c: Downloading [=============>                                     ]  78.97MB/299.9MB\n",
        "# db0eeba8da4c: Pull complete \n",
        "# b9298a86fbdf: Pull complete \n",
        "# bf8c7b136b1c: Pull complete \n",
        "# e6fe1f265d0a: Pull complete \n",
        "# 640d4d39aaea: Pull complete \n",
        "# 100eef49d013: Pull complete \n",
        "# ed7af753a2e0: Pull complete \n",
        "# d510e034f1d4: Pull complete \n",
        "# 78e6442cd03c: Pull complete \n",
        "# d268813d3f35: Pull complete \n",
        "# aab58f3de097: Pull complete \n",
        "# 677c296b202b: Pull complete \n",
        "# 3aac3593beee: Pull complete \n",
        "# f98df7a44116: Pull complete \n",
        "# aeb5c0ac0503: Pull complete \n",
        "# 22cc0ad6f77b: Pull complete \n",
        "# 1bf01ffb4b82: Pull complete \n",
        "# b67889547497: Pull complete \n",
        "# 632e856b088f: Pull complete \n",
        "# cf3a603af2b6: Pull complete \n",
        "# f1c3cc4680ba: Pull complete \n",
        "# a8071a53e4d9: Pull complete \n",
        "# 25f2f99f0427: Pull complete \n",
        "# afdf3995ef29: Pull complete \n",
        "# 5739c580f05e: Pull complete \n",
        "# f3001dbd2517: Pull complete \n",
        "# Digest: sha256:d54adaa3fe743595fef711fea8716a2403c8b3749b539d811e2b2fe0c292aa8c\n",
        "# Status: Downloaded newer image for sindq/sb-m8:latest\n",
        "# 96b73acfe24ec956d9079bb8608c845db7d0092c559d68e6ae8e1d6f9d10c16a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# root@96b73acfe24e:~# ls -l\n",
        "# total 24\n",
        "# -rwxrwxrwx 1 root   users    0 Apr 29 19:38 access.log\n",
        "# -rwxrwxrwx 1 root   users  611 Apr 29 19:38 error.log\n",
        "# -rwxrwxrwx 1 root   users 1001 Apr 29 19:38 jupyter.log\n",
        "# -rwxrwxrwx 1 root   users    0 Apr 29 19:38 nginx.log\n",
        "# -rwxrwxr-x 1 root   root   496 Nov 17  2022 start-hadoop.sh\n",
        "# -rw-r--r-- 1 root   users  664 Apr 29 19:39 supervisord.log\n",
        "# -rw-r--r-- 1 root   users    5 Apr 29 19:38 supervisord.pid\n",
        "# drwsrwsr-x 1 jovyan users 4096 Nov 17  2022 work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qZPvlMmJsQCK"
      },
      "outputs": [],
      "source": [
        "# Заходим в контейнер\n",
        "# docker exec -it ecstatic_lichterman /bin/bash\n",
        "\n",
        "# admin@hadoopvm:~$ docker exec -it ecstatic_lichterman /bin/bash\n",
        "# root@96b73acfe24e:~#\n",
        "\n",
        "# exit\n",
        "# admin@hadoopvm:~$\n",
        "\n",
        "\n",
        "# Файлы в контейнере\n",
        "# admin@hadoopvm:~$ sudo find / -name log_mini.csv\n",
        "# /var/lib/docker/overlay2/078be585b41d0d4b6e4b5dac5000bed94a5b5015c449531111ba447e8d910ca8/diff/home/jovyan/work/spotify/log_mini.csv\n",
        "# /var/lib/docker/overlay2/a9a6cd24d896b33f3b34e211593949c4ed9814bcbf61037eb2e434069e53de4e/merged/home/jovyan/work/spotify/log_mini.csv\n",
        "# /var/lib/docker/overlay2/ec14a7899e807fbf4761bf4fdbcaa95492c107cb0e21b79e3ddeb96bcff4d167/merged/home/jovyan/work/spotify/log_mini.csv\n",
        "# /var/lib/docker/overlay2/527c1533ab5e18f67567a219f8d1e85ba65e7094005cbd419bf444ffe663fda5/diff/home/jovyan/work/spotify/log_mini.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# docker exec ecstatic_lichterman cat /home/jovyan/work/spotify/log_mini.csv\n",
        "\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,5,20,t_f673e1b7-4ebe-4fc1-ac24-a9f25de70381,false,false,false,true,0,1,0,0,0,0,false,12,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,6,20,t_e172e8e7-7161-42a9-acb0-d606346c8f87,false,false,false,true,0,1,0,0,0,0,false,12,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,7,20,t_77977dd6-597e-4425-8f8f-4efb32ecfba6,false,false,false,true,0,1,0,0,0,0,false,12,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,8,20,t_96a13618-10dc-498e-9de5-0940e4c84605,false,false,false,true,0,1,0,0,0,0,false,12,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,9,20,t_356f276e-ffac-4ff1-9c1b-0f59f402dc5f,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,10,20,t_e880d88c-bc6d-4f29-a220-b2d75aaade43,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,11,20,t_773a9f8f-2d00-4467-965a-df709d650bfd,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,12,20,t_94c1dd31-2410-4167-a75b-d6433d20bb04,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,13,20,t_bda3bb62-b717-4766-b417-4e7b0a31579b,true,true,true,false,0,0,0,1,0,0,false,13,2018-07-15,true,user_collection,trackdone,fwdbtn\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,14,20,t_f80ee4db-b284-4ef7-bdde-604ea6354f3c,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,fwdbtn,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,15,20,t_e76ba594-cebd-472d-a2da-483568be0406,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,16,20,t_360910e8-2a84-42b0-baf1-59abcf96a1f2,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,17,20,t_aa2fff77-9b0a-4fa3-a685-ecef50310e8a,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,18,20,t_f673e1b7-4ebe-4fc1-ac24-a9f25de70381,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,19,20,t_e172e8e7-7161-42a9-acb0-d606346c8f87,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone\n",
        "# 0_0eaeef5d-25e9-4429-bd55-af15d3604c9f,20,20,t_77977dd6-597e-4425-8f8f-4efb32ecfba6,false,false,false,true,0,1,0,0,0,0,false,13,2018-07-15,true,user_collection,trackdone,trackdone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nyJ_kdCnsQCK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing data/mapper.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data/mapper.py\n",
        "import sys\n",
        "\n",
        "# Чтение каждой строки из стандартного ввода\n",
        "for line in sys.stdin:\n",
        "    # Очистка строки от лишних пробелов и переносов строк\n",
        "    line = line.strip()\n",
        "    \n",
        "    # Избегаем обработку заголовка или пустой строки\n",
        "    if line and not line.startswith('session_id'):\n",
        "        # Разделение строки на части по запятой\n",
        "        parts = line.split(',')\n",
        "        \n",
        "        # Извлечение 'track_id_clean', который находится в четвертом столбце (индекс 3)\n",
        "        track_id_clean = parts[3]\n",
        "        \n",
        "        # Вывод 'track_id_clean' и 1, разделенные табуляцией\n",
        "        print(f\"{track_id_clean}\\t1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Копируем на локальную машину\n",
        "\n",
        "# admin@hadoopvm:~$ docker cp ecstatic_lichterman:/home/jovyan/work/spotify/log_mini.csv /tmp/log_mini.csv\n",
        "# Successfully copied 28.9MB to /tmp/log_mini.csv\n",
        "\n",
        "# (.venv) ➜  NoSQL_Big_Data git:(main) ✗ scp -i /Users/andrejizvarin/Documents/___project/NoSQL_Big_Data/data/sshyandex admin@158.160.153.0:/tmp/log_mini.csv /Users/andrejizvarin/Documents/___project/NoSQL_Big_Data/data/log_mini.csv\n",
        "# Enter passphrase for key '/Users/andrejizvarin/Documents/___project/NoSQL_Big_Data/data/sshyandex': \n",
        "# log_mini.csv    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5LsMIUzQsQCL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t_0479f24c-27d2-46d6-a00c-7ec928f2b539\t1\n",
            "t_9099cd7b-c238-47b7-9381-f23f2c1d1043\t1\n",
            "t_fc5df5ba-5396-49a7-8b29-35d0d28249e0\t1\n",
            "t_23cff8d6-d874-4b20-83dc-94e450e8aa20\t1\n"
          ]
        }
      ],
      "source": [
        "# Протестируйте mapper локально\n",
        "! head -n 5 data/log_mini.csv | python data/mapper.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xPZUhcizsQCL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing data/reducer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data/reducer.py\n",
        "import sys\n",
        "\n",
        "current_track_id = None\n",
        "current_count = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    track_id, count = line.split('\\t', 1)\n",
        "    count = int(count)\n",
        "\n",
        "    if current_track_id == track_id:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_track_id:\n",
        "            print(f\"{current_track_id}\\t{current_count}\")\n",
        "        current_track_id = track_id\n",
        "        current_count = count\n",
        "\n",
        "if current_track_id == track_id:\n",
        "    print(f\"{current_track_id}\\t{current_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tOePV0eFsQCM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aa\t10\n",
            "bb\t5\n"
          ]
        }
      ],
      "source": [
        "# Протестируйте reducer локально\n",
        "! python -c \"print('\\n'.join([f'{x}\\t1' for x in (['aa'] * 10 + ['bb'] * 5)]))\" | python data/reducer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Перенос mapper.py и reducer.py в контейнер ecstatic_lichterman\n",
        "\n",
        "# scp -i data/sshyandex data/mapper.py admin@158.160.153.0:/tmp\n",
        "# scp -i data/sshyandex data/reducer.py admin@158.160.153.0:/tmp\n",
        "# scp -i data/sshyandex data/log_mini.csv admin@158.160.153.0:/tmp\n",
        "\n",
        "# (.venv) ➜  NoSQL_Big_Data git:(main) ✗ scp -i data/sshyandex /Users/andrejizvarin/Documents/___project/NoSQL_Big_Data/data/mapper.py admin@158.160.153.0:/tmp\n",
        "# Enter passphrase for key 'data/sshyandex':\n",
        "# mapper.py                                                                                                                                         100%  820    51.4KB/s   00:00\n",
        "# (.venv) ➜  NoSQL_Big_Data git:(main) ✗ scp -i data/sshyandex /Users/andrejizvarin/Documents/___project/NoSQL_Big_Data/data/reducer.py admin@158.160.153.0:/tmp\n",
        "# Enter passphrase for key 'data/sshyandex':\n",
        "# reducer.py\n",
        "# (.venv) ➜  NoSQL_Big_Data git:(main) ✗ scp -i data/sshyandex data/log_mini.csv admin@158.160.153.0:/tmp\n",
        "# Enter passphrase for key 'data/sshyandex':\n",
        "# log_mini.csv\n",
        "\n",
        "# docker cp /tmp/mapper.py ecstatic_lichterman:/home/jovyan\n",
        "# docker cp /tmp/reducer.py ecstatic_lichterman:/home/jovyan\n",
        "# docker cp /tmp/log_mini.csv ecstatic_lichterman:/home/jovyan\n",
        "\n",
        "# admin@hadoopvm:~$ docker cp /tmp/mapper.py ecstatic_lichterman:/home/jovyan\n",
        "# Successfully copied 2.56kB to ecstatic_lichterman:/home/jovyan\n",
        "# admin@hadoopvm:~$ docker cp /tmp/reducer.py ecstatic_lichterman:/home/jovyan\n",
        "# Successfully copied 2.05kB to ecstatic_lichterman:/home/jovyan\n",
        "# admin@hadoopvm:~$ docker cp /tmp/log_mini.csv ecstatic_lichterman:/home/jovyan\n",
        "# Successfully copied 28.9MB to ecstatic_lichterman:/home/jovyan\n",
        "\n",
        "\n",
        "\n",
        "# Переносим log_mini.csv в hdfs\n",
        "\n",
        "# docker exec -it ecstatic_lichterman /bin/bash\n",
        "\n",
        "# hdfs dfs -mkdir -p /user/root/\n",
        "# hdfs dfs -put /home/jovyan/log_mini.csv /user/root/log_mini.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w11D2hT3sQCN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: mapred\n"
          ]
        }
      ],
      "source": [
        "# Запустите MapReduce Streaming\n",
        "! mapred streaming \\\n",
        "  -input log_mini.csv \\\n",
        "  -output /track-count \\\n",
        "  -mapper \"/opt/conda/bin/python mapper.py\" \\\n",
        "  -reducer \"/opt/conda/bin/python reducer.py\" \\\n",
        "  -file mapper.py \\\n",
        "  -file reducer.py\n",
        "\n",
        "\n",
        "#   root@96b73acfe24e:~# ! mapred streaming \\\n",
        "# >   -input log_mini.csv \\\n",
        "# >   -output /track-count \\\n",
        "# >   -mapper \"/opt/conda/bin/python mapper.py\" \\\n",
        "# >   -reducer \"/opt/conda/bin/python reducer.py\" \\\n",
        "# >   -file mapper.py \\\n",
        "# >   -file reducer.py\n",
        "# 2024-04-29 21:31:18,501 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
        "# packageJobJar: [mapper.py, reducer.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar] /tmp/streamjob173635931369528032.jar tmpDir=null\n",
        "# 2024-04-29 21:31:19,330 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
        "# 2024-04-29 21:31:19,482 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
        "# 2024-04-29 21:31:19,698 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1714419532068_0006\n",
        "# 2024-04-29 21:31:19,987 INFO mapred.FileInputFormat: Total input files to process : 1\n",
        "# 2024-04-29 21:31:20,010 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:9866\n",
        "# 2024-04-29 21:31:20,059 INFO mapreduce.JobSubmitter: number of splits:2\n",
        "# 2024-04-29 21:31:20,185 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1714419532068_0006\n",
        "# 2024-04-29 21:31:20,185 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
        "# 2024-04-29 21:31:20,337 INFO conf.Configuration: resource-types.xml not found\n",
        "# 2024-04-29 21:31:20,337 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
        "# 2024-04-29 21:31:20,611 INFO impl.YarnClientImpl: Submitted application application_1714419532068_0006\n",
        "# 2024-04-29 21:31:20,660 INFO mapreduce.Job: The url to track the job: http://96b73acfe24e:8088/proxy/application_1714419532068_0006/\n",
        "# 2024-04-29 21:31:20,661 INFO mapreduce.Job: Running job: job_1714419532068_0006\n",
        "# 2024-04-29 21:31:27,768 INFO mapreduce.Job: Job job_1714419532068_0006 running in uber mode : false\n",
        "# 2024-04-29 21:31:27,769 INFO mapreduce.Job:  map 0% reduce 0%\n",
        "# 2024-04-29 21:31:32,828 INFO mapreduce.Job:  map 50% reduce 0%\n",
        "# 2024-04-29 21:31:33,833 INFO mapreduce.Job:  map 100% reduce 0%\n",
        "# 2024-04-29 21:31:38,862 INFO mapreduce.Job:  map 100% reduce 100%\n",
        "# 2024-04-29 21:31:38,869 INFO mapreduce.Job: Job job_1714419532068_0006 completed successfully\n",
        "# 2024-04-29 21:31:38,948 INFO mapreduce.Job: Counters: 55\n",
        "#         File System Counters\n",
        "#                 FILE: Number of bytes read=7218846\n",
        "#                 FILE: Number of bytes written=15275927\n",
        "#                 FILE: Number of read operations=0\n",
        "#                 FILE: Number of large read operations=0\n",
        "#                 FILE: Number of write operations=0\n",
        "#                 HDFS: Number of bytes read=28922958\n",
        "#                 HDFS: Number of bytes written=2081227\n",
        "#                 HDFS: Number of read operations=11\n",
        "#                 HDFS: Number of large read operations=0\n",
        "#                 HDFS: Number of write operations=2\n",
        "#                 HDFS: Number of bytes read erasure-coded=0\n",
        "#         Job Counters \n",
        "#                 Launched map tasks=2\n",
        "#                 Launched reduce tasks=1\n",
        "#                 Data-local map tasks=1\n",
        "#                 Rack-local map tasks=1\n",
        "#                 Total time spent by all maps in occupied slots (ms)=3521024\n",
        "#                 Total time spent by all reduces in occupied slots (ms)=2769920\n",
        "#                 Total time spent by all map tasks (ms)=6877\n",
        "#                 Total time spent by all reduce tasks (ms)=2705\n",
        "#                 Total vcore-milliseconds taken by all map tasks=6877\n",
        "#                 Total vcore-milliseconds taken by all reduce tasks=2705\n",
        "#                 Total megabyte-milliseconds taken by all map tasks=3521024\n",
        "#                 Total megabyte-milliseconds taken by all reduce tasks=2769920\n",
        "#         Map-Reduce Framework\n",
        "#                 Map input records=167881\n",
        "#                 Map output records=167880\n",
        "#                 Map output bytes=6883080\n",
        "#                 Map output materialized bytes=7218852\n",
        "#                 Input split bytes=192\n",
        "#                 Combine input records=0\n",
        "#                 Combine output records=0\n",
        "#                 Reduce input groups=50704\n",
        "#                 Reduce shuffle bytes=7218852\n",
        "#                 Reduce input records=167880\n",
        "#                 Reduce output records=50704\n",
        "#                 Spilled Records=335760\n",
        "#                 Shuffled Maps =2\n",
        "#                 Failed Shuffles=0\n",
        "#                 Merged Map outputs=2\n",
        "#                 GC time elapsed (ms)=216\n",
        "#                 CPU time spent (ms)=4750\n",
        "#                 Physical memory (bytes) snapshot=806109184\n",
        "#                 Virtual memory (bytes) snapshot=6896967680\n",
        "#                 Total committed heap usage (bytes)=722993152\n",
        "#                 Peak Map Physical memory (bytes)=303341568\n",
        "#                 Peak Map Virtual memory (bytes)=2150793216\n",
        "#                 Peak Reduce Physical memory (bytes)=206458880\n",
        "#                 Peak Reduce Virtual memory (bytes)=2600173568\n",
        "#         Shuffle Errors\n",
        "#                 BAD_ID=0\n",
        "#                 CONNECTION=0\n",
        "#                 IO_ERROR=0\n",
        "#                 WRONG_LENGTH=0\n",
        "#                 WRONG_MAP=0\n",
        "#                 WRONG_REDUCE=0\n",
        "#         File Input Format Counters \n",
        "#                 Bytes Read=28922766\n",
        "#         File Output Format Counters \n",
        "#                 Bytes Written=2081227\n",
        "# 2024-04-29 21:31:38,948 INFO streaming.StreamJob: Output directory: /track-count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gTdry0IisQCN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting data/stream_processor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data/stream_processor.py\n",
        "import sys\n",
        "import json\n",
        "\n",
        "# Подзадачи:\n",
        "# 1. Подсчитайте количество уникальных треков:\n",
        "uniq_tracks_count = 0\n",
        "uniq_tracks = set()\n",
        "\n",
        "# 2. Посчитайте количество треков с количеством прослушиваний больше 20:\n",
        "popular_tracks_count = 0\n",
        "\n",
        "# 3. Найдите два самых популярных по listen_count:\n",
        "top_2_tracks = [(None, 0), (None, 0)]  # (track_id, listen_count)\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # line - результат работы MapReduce, формат от reducer.py - <track_id>\\t<N>\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "        track_id, count = line.split('\\t')\n",
        "        count = int(count)\n",
        "\n",
        "        # Обновляем множество уникальных треков\n",
        "        uniq_tracks.add(track_id)\n",
        "\n",
        "        # Подсчитываем треки с прослушиваниями больше 20\n",
        "        if count > 20:\n",
        "            popular_tracks_count += 1\n",
        "\n",
        "        # Находим топ-2 трека\n",
        "        if count > top_2_tracks[0][1]:\n",
        "            top_2_tracks[1] = top_2_tracks[0]\n",
        "            top_2_tracks[0] = (track_id, count)\n",
        "        elif count > top_2_tracks[1][1]:\n",
        "            top_2_tracks[1] = (track_id, count)\n",
        "\n",
        "# Финальные значения для результатов\n",
        "uniq_tracks_count = len(uniq_tracks)\n",
        "\n",
        "data = {\n",
        "    'q1': uniq_tracks_count,\n",
        "    'q2': popular_tracks_count,\n",
        "    'q3': top_2_tracks,\n",
        "}\n",
        "\n",
        "# Сохраняем результаты в файл\n",
        "with open('result.json', 'w') as f:\n",
        "    f.write(json.dumps(data, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Перенос stream_processor.py в контейнер ecstatic_lichterman\n",
        "\n",
        "# scp -i data/sshyandex data/stream_processor.py admin@158.160.153.0:/tmp\n",
        "# (.venv) ➜  NoSQL_Big_Data git:(main) ✗ scp -i data/sshyandex data/stream_processor.py admin@158.160.153.0:/tmp\n",
        "# Enter passphrase for key 'data/sshyandex':\n",
        "# stream_processor.py\n",
        "\n",
        "# docker cp /tmp/stream_processor.py ecstatic_lichterman:/home/jovyan\n",
        "# admin@hadoopvm:~$ docker cp /tmp/stream_processor.py ecstatic_lichterman:/home/jovyan\n",
        "# Successfully copied 3.58kB to ecstatic_lichterman:/home/jovyan\n",
        "\n",
        "# docker exec -it ecstatic_lichterman /bin/bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp8Upyk7sQCO"
      },
      "outputs": [],
      "source": [
        "# Обработайте данные из HDFS с помощью stream_processor.py\n",
        "! hdfs dfs -cat /track-count/* | python stream_processor.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwy4VsPBsQCO"
      },
      "outputs": [],
      "source": [
        "# Выведите содержимое файла result.json\n",
        "! cat result.json\n",
        "\n",
        "\n",
        "# admin@hadoopvm:~$ docker exec -it ecstatic_lichterman /bin/bash\n",
        "# root@96b73acfe24e:~# ! hdfs dfs -cat /track-count/* | python stream_processor.py\n",
        "# root@96b73acfe24e:~# ls -l\n",
        "# total 28284\n",
        "# -rwxrwxrwx 1 root   users        0 Apr 29 19:38 access.log\n",
        "# -rwxrwxrwx 1 root   users      611 Apr 29 19:38 error.log\n",
        "# -rwxrwxrwx 1 root   users     1001 Apr 29 19:38 jupyter.log\n",
        "# -rw-rw-r-- 1 jovyan  1003 28918670 Apr 29 21:22 log_mini.csv\n",
        "# -rw-r--r-- 1 jovyan  1003      820 Apr 29 21:10 mapper.py\n",
        "# -rwxrwxrwx 1 root   users        0 Apr 29 19:38 nginx.log\n",
        "# -rw-r--r-- 1 jovyan  1003      483 Apr 29 21:10 reducer.py\n",
        "# -rw-r--r-- 1 root   users      235 Apr 29 21:44 result.json\n",
        "# -rwxrwxr-x 1 root   root       496 Nov 17  2022 start-hadoop.sh\n",
        "# -rw-r--r-- 1 jovyan  1003     1584 Apr 29 21:43 stream_processor.py\n",
        "# -rw-r--r-- 1 root   users      664 Apr 29 19:39 supervisord.log\n",
        "# -rw-r--r-- 1 root   users        5 Apr 29 19:38 supervisord.pid\n",
        "# drwsrwsr-x 1 jovyan users     4096 Nov 17  2022 work\n",
        "# root@96b73acfe24e:~# ! cat result.json\n",
        "# {\n",
        "#     \"q1\": 50704,\n",
        "#     \"q2\": 874,\n",
        "#     \"q3\": [\n",
        "#         [\n",
        "#             \"t_bacf06d3-9185-4183-84ea-ff0db51475ce\",\n",
        "#             1427\n",
        "#         ],\n",
        "#         [\n",
        "#             \"t_5718ab08-3a15-4d3f-9e63-42b2f6805e31\",\n",
        "#             915\n",
        "#         ]\n",
        "#     ]\n",
        "# }root@96b73acfe24e:~# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "q1: 50704 - Это количество различных треков, которые были обработаны. Это число показывает, сколько уникальных треков есть в данных.\n",
        "q2: 874 - Столько треков было прослушано более 20 раз. Это показатель того, какие треки особенно популярны.\n",
        "q3: [['t_bacf06d3-9185-4183-84ea-ff0db51475ce', 1427], ['t_5718ab08-3a15-4d3f-9e63-42b2f6805e31', 915]] - Это два самых популярных трека. Первый был прослушан 1427 раз, второй — 915 раз. Это самые популярные треки по количеству прослушиваний."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
