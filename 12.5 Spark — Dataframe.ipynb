{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark не установлен. Устанавливаем...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/andrejizvarin/Library/Python/3.9/lib/python/site-packages (from pyspark) (0.10.9.7)\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "\n",
    "# Функция для установки пакета\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "# Проверка и установка пакетов\n",
    "required_packages = {\n",
    "    \"pyspark\": \"pyspark\",\n",
    "}\n",
    "\n",
    "for package, pip_name in required_packages.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} не установлен. Устанавливаем...\")\n",
    "        install(pip_name)\n",
    "    else:\n",
    "        print(f\"{package} уже установлен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  # type: ignore\n",
    "from pyspark.sql.functions import lit, col, sum, udf  # type: ignore\n",
    "from pyspark.sql.types import StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы успешно разархивированы\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/06 03:32:26 WARN Utils: Your hostname, MacBook-Pro-Andrej.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "24/05/06 03:32:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/andrejizvarin/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/andrejizvarin/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-18de887c-9bf3-49f2-96c9-ae72875a475a;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/andrejizvarin/Library/Python/3.9/lib/python/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-avro_2.12;3.5.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      ":: resolution report :: resolve 113ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.1 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-18de887c-9bf3-49f2-96c9-ae72875a475a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "24/05/06 03:32:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      " |-- Month: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/06 03:32:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+---------+\n",
      "|summary|         Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|    Month|\n",
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+---------+\n",
      "|  count|           185950|      186305|             185950|            185950|        186305|              186305|   186850|\n",
      "|   mean|230417.5693788653|        NULL| 1.1243828986286637|184.39973476743927|          NULL|                NULL|     NULL|\n",
      "| stddev|51512.73710999595|        NULL|0.44279262402866953| 332.7313298843437|          NULL|                NULL|     NULL|\n",
      "|    min|           141234|20in Monitor|                  1|              2.99|01/01/19 03:07|1 11th St, Atlant...|    April|\n",
      "|    max|           319670|      iPhone|                  9|            1700.0|    Order Date|    Purchase Address|September|\n",
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+---------+\n",
      "\n",
      "+--------------------+-----+\n",
      "|             Product|count|\n",
      "+--------------------+-----+\n",
      "|USB-C Charging Cable|21903|\n",
      "|Lightning Chargin...|21658|\n",
      "|AAA Batteries (4-...|20641|\n",
      "|AA Batteries (4-p...|20577|\n",
      "|    Wired Headphones|18882|\n",
      "|Apple Airpods Hea...|15549|\n",
      "|Bose SoundSport H...|13325|\n",
      "|    27in FHD Monitor| 7507|\n",
      "|              iPhone| 6842|\n",
      "|27in 4K Gaming Mo...| 6230|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1: Загрузка и анализ данных из CSV-файла\n",
    "\n",
    "# Путь, куда будут извлечены файлы\n",
    "data_folder = \"data/sales_data\"\n",
    "\n",
    "# Создаем папку, если она не существует\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Разархивирование файла\n",
    "with zipfile.ZipFile(\"data/sales_data.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_folder)\n",
    "print(\"Файлы успешно разархивированы\")\n",
    "\n",
    "\n",
    "# Установка переменной среды PYSPARK_SUBMIT_ARGS\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.spark:spark-avro_2.12:3.5.1 pyspark-shell\"\n",
    ")\n",
    "\n",
    "# Создание сессии Spark\n",
    "spark = SparkSession.builder.appName(\"Sales Data Analysis\").getOrCreate()\n",
    "\n",
    "\n",
    "# Получение списка файлов в директории\n",
    "data_files = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Инициализация пустого DataFrame\n",
    "full_df = None\n",
    "\n",
    "# Чтение и объединение всех файлов\n",
    "for file in data_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    month = file.split(\"_\")[1]  # Извлечение месяца из имени файла\n",
    "    month_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    month_df = month_df.withColumn(\"Month\", lit(month))\n",
    "\n",
    "    if full_df is None:\n",
    "        full_df = month_df\n",
    "    else:\n",
    "        full_df = full_df.union(month_df)\n",
    "\n",
    "# Вывод схемы DataFrame\n",
    "full_df.printSchema()\n",
    "\n",
    "# Вывод общей статистики по данным\n",
    "full_df.describe().show()\n",
    "\n",
    "# Анализ самых популярных товаров\n",
    "top_products = full_df.groupBy(\"Product\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "# Вывод Топ-10 самых популярных товаров\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=39728Kb max_used=39741Kb free=91343Kb\n",
      " bounds [0x00000001071e8000, 0x00000001098f8000, 0x000000010f1e8000]\n",
      " total_blobs=13804 nmethods=12836 adapters=880\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения для непартиционированного DataFrame: 0.49 секунд\n",
      "Время выполнения для партиционированного DataFrame: 0.39 секунд\n",
      "Улучшение производительности: 1.2386262352257313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2: Партиционирование данных и оптимизация производительности\n",
    "\n",
    "# Партиционирование DataFrame по столбцу 'Month'\n",
    "partitioned_df = full_df.repartition(\"Month\")\n",
    "\n",
    "# Выполнение агрегации: подсчет общего количества проданных товаров по месяцам\n",
    "aggregated_df = partitioned_df.groupBy(\"Month\").agg(\n",
    "    sum(\"Quantity Ordered\").alias(\"Total Quantity Sold\")\n",
    ")\n",
    "\n",
    "\n",
    "# Оценка производительности\n",
    "# Функция для замера времени выполнения\n",
    "def measure_performance(df):\n",
    "    start_time = time.time()\n",
    "    df.groupBy(\"Month\").agg(\n",
    "        sum(\"Quantity Ordered\").alias(\"Total Quantity Sold\")\n",
    "    ).collect()\n",
    "    return time.time() - start_time\n",
    "\n",
    "\n",
    "# Измерение времени для непартиционированного DataFrame\n",
    "unpartitioned_time = measure_performance(full_df)\n",
    "print(\n",
    "    f\"Время выполнения для непартиционированного DataFrame: {unpartitioned_time:.2f} секунд\"\n",
    ")\n",
    "\n",
    "# Измерение времени для партиционированного DataFrame\n",
    "partitioned_time = measure_performance(partitioned_df)\n",
    "print(\n",
    "    f\"Время выполнения для партиционированного DataFrame: {partitioned_time:.2f} секунд\"\n",
    ")\n",
    "\n",
    "# Сравнение результатов\n",
    "print(\"Улучшение производительности:\", unpartitioned_time / partitioned_time)\n",
    "\n",
    "# Изменение количества партиций на одну и сохранение на диск\n",
    "single_partition_df = partitioned_df.coalesce(1)\n",
    "single_partition_df.write.format(\"csv\").mode(\"overwrite\").save(\n",
    "    \"data/single_partitioned_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/06 03:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/05/06 03:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/05/06 03:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 75,08% for 9 writers\n",
      "24/05/06 03:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 67,58% for 10 writers\n",
      "24/05/06 03:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 75,08% for 9 writers\n",
      "24/05/06 03:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/05/06 03:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время записи в формат json: 0.38 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/06 03:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/05/06 03:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/05/06 03:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 75,08% for 9 writers\n",
      "24/05/06 03:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 67,58% for 10 writers\n",
      "24/05/06 03:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 75,08% for 9 writers\n",
      "24/05/06 03:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/05/06 03:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время записи в формат parquet: 0.41 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время записи в формат orc: 0.74 секунд\n",
      "Время записи в формат avro: 0.33 секунд\n"
     ]
    }
   ],
   "source": [
    "# 3: Работа с различными форматами данных\n",
    "\n",
    "# Путь для сохранения файлов\n",
    "output_path = \"data/output\"\n",
    "\n",
    "# Удаление пробелов из названий столбцов для совместимости с форматом Avro\n",
    "fixed_df = full_df.select([col(c).alias(c.replace(\" \", \"_\")) for c in full_df.columns])\n",
    "\n",
    "# Сохранение в формате JSON\n",
    "fixed_df.write.mode(\"overwrite\").json(f\"{output_path}/data.json\")\n",
    "\n",
    "# Сохранение в формате Parquet\n",
    "fixed_df.write.mode(\"overwrite\").parquet(f\"{output_path}/data.parquet\")\n",
    "\n",
    "# Сохранение в формате ORC\n",
    "fixed_df.write.mode(\"overwrite\").orc(f\"{output_path}/data.orc\")\n",
    "\n",
    "# Сохранение в формате Avro\n",
    "fixed_df.write.format(\"avro\").mode(\"overwrite\").save(f\"{output_path}/data.avro\")\n",
    "\n",
    "\n",
    "# Функция для замера времени записи DataFrame в определенный формат\n",
    "def measure_write_performance(df, format, path):\n",
    "    start_time = time.time()\n",
    "    if format == \"json\":\n",
    "        df.write.mode(\"overwrite\").json(path)\n",
    "    elif format == \"parquet\":\n",
    "        df.write.mode(\"overwrite\").parquet(path)\n",
    "    elif format == \"orc\":\n",
    "        df.write.mode(\"overwrite\").orc(path)\n",
    "    elif format == \"avro\":\n",
    "        df.write.format(\"avro\").mode(\"overwrite\").save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format\")\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Время записи в формат {format}: {elapsed_time:.2f} секунд\")\n",
    "\n",
    "\n",
    "# Замер производительности для каждого формата\n",
    "measure_write_performance(fixed_df, \"json\", f\"{output_path}/data.json\")\n",
    "measure_write_performance(fixed_df, \"parquet\", f\"{output_path}/data.parquet\")\n",
    "measure_write_performance(fixed_df, \"orc\", f\"{output_path}/data.orc\")\n",
    "measure_write_performance(fixed_df, \"avro\", f\"{output_path}/data.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------+----------------+-------------+-----+-----------+-----------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|   Month|          Street|         City|State|Postal_Code|Total Price|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------+----------------+-------------+-----+-----------+-----------+\n",
      "|  295665|  Macbook Pro Laptop|               1|    1700.0|12/30/19 00:01|136 Church St, Ne...|December|   136 Church St|New York City|   NY|      10001|     1700.0|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|12/29/19 07:03|562 2nd St, New Y...|December|      562 2nd St|New York City|   NY|      10001|      600.0|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|12/12/19 18:21|277 Main St, New ...|December|     277 Main St|New York City|   NY|      10001|      11.95|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|12/22/19 15:13|410 6th St, San F...|December|      410 6th St|San Francisco|   CA|      94016|     149.99|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|12/18/19 12:38|43 Hill St, Atlan...|December|      43 Hill St|      Atlanta|   GA|      30301|      11.95|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|12/31/19 22:58|200 Jefferson St,...|December|200 Jefferson St|New York City|   NY|      10001|       3.84|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|12/16/19 15:10|928 12th St, Port...|December|     928 12th St|     Portland|   OR|      97035|      11.95|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|12/13/19 09:29|813 Hickory St, D...|December|  813 Hickory St|       Dallas|   TX|      75001|       23.9|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|12/15/19 23:26|718 Wilson St, Da...|December|   718 Wilson St|       Dallas|   TX|      75001|      99.99|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|12/28/19 11:51|77 7th St, Dallas...|December|       77 7th St|       Dallas|   TX|      75001|      11.96|\n",
      "|  295675|USB-C Charging Cable|               2|     11.95|12/13/19 13:52|594 1st St, San F...|December|      594 1st St|San Francisco|   CA|      94016|       23.9|\n",
      "|  295676|     ThinkPad Laptop|               1|    999.99|12/28/19 17:19|410 Lincoln St, L...|December|  410 Lincoln St|  Los Angeles|   CA|      90001|     999.99|\n",
      "|  295677|AA Batteries (4-p...|               2|      3.84|12/20/19 19:19|866 Pine St, Bost...|December|     866 Pine St|       Boston|   MA|      02215|       7.68|\n",
      "|  295678|AAA Batteries (4-...|               2|      2.99|12/06/19 09:38|187 Lincoln St, D...|December|  187 Lincoln St|       Dallas|   TX|      75001|       5.98|\n",
      "|  295679|USB-C Charging Cable|               1|     11.95|12/25/19 09:39|902 2nd St, Dalla...|December|      902 2nd St|       Dallas|   TX|      75001|      11.95|\n",
      "|  295680|Lightning Chargin...|               1|     14.95|12/01/19 14:30|338 Main St, Aust...|December|     338 Main St|       Austin|   TX|      73301|      14.95|\n",
      "|  295681|        Google Phone|               1|     600.0|12/25/19 12:37|79 Elm St, Boston...|December|       79 Elm St|       Boston|   MA|      02215|      600.0|\n",
      "|  295681|USB-C Charging Cable|               1|     11.95|12/25/19 12:37|79 Elm St, Boston...|December|       79 Elm St|       Boston|   MA|      02215|      11.95|\n",
      "|  295681|Bose SoundSport H...|               1|     99.99|12/25/19 12:37|79 Elm St, Boston...|December|       79 Elm St|       Boston|   MA|      02215|      99.99|\n",
      "|  295681|    Wired Headphones|               1|     11.99|12/25/19 12:37|79 Elm St, Boston...|December|       79 Elm St|       Boston|   MA|      02215|      11.99|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------+----------------+-------------+-----+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выполнение с кешированием: 2.41 секунд\n",
      "Выполнение без кеширования: 1.14 секунд\n"
     ]
    }
   ],
   "source": [
    "# 4: Работа с UDF и кешированием\n",
    "\n",
    "\n",
    "def parse_address(address):\n",
    "    if address is None:\n",
    "        return (\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )  # Возвращаем кортеж с None для каждой части адреса\n",
    "    parts = address.split(\",\")\n",
    "    if len(parts) < 3:\n",
    "        return (None, None, None, None)  # Не достаточно данных для разбиения адреса\n",
    "    street = parts[0].strip()\n",
    "    city = parts[1].strip()\n",
    "    state_postal = parts[2].strip().split(\" \")\n",
    "    state = state_postal[0] if len(state_postal) > 1 else None\n",
    "    postal_code = state_postal[1] if len(state_postal) > 1 else None\n",
    "    return (street, city, state, postal_code)\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"Street\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True),\n",
    "        StructField(\"Postal_Code\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parse_address_udf = udf(parse_address, schema)\n",
    "\n",
    "# Применение UDF\n",
    "address_df = full_df.withColumn(\n",
    "    \"Parsed_Address\", parse_address_udf(col(\"Purchase Address\"))\n",
    ")\n",
    "address_df = address_df.select(\"*\", \"Parsed_Address.*\").drop(\"Parsed_Address\")\n",
    "\n",
    "# Кеширование для улучшения производительности\n",
    "address_df.cache()\n",
    "\n",
    "# Добавление столбца с общей ценой\n",
    "address_df = address_df.withColumn(\n",
    "    \"Total Price\", col(\"Quantity Ordered\") * col(\"Price Each\")\n",
    ")\n",
    "\n",
    "# Запуск действий для активации кеширования и оценки результатов\n",
    "address_df.show()\n",
    "\n",
    "\n",
    "def measure_performance(operation_description, func):\n",
    "    start_time = time.time()\n",
    "    func()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"{operation_description}: {elapsed_time:.2f} секунд\")\n",
    "\n",
    "\n",
    "measure_performance(\"Выполнение с кешированием\", lambda: address_df.collect())\n",
    "address_df.unpersist()  # Очистка кеша\n",
    "measure_performance(\"Выполнение без кеширования\", lambda: address_df.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример, где кеширование полезно\n",
    "Кеширование часто используется в веб-разработке для ускорения загрузки страниц, сохраняя данные, часто запрашиваемые пользователями, в памяти.\n",
    "Например, на новостных сайтах, где статьи обновляются нечасто, но читаются многими пользователями, кеширование позволяет сократить нагрузку на сервер и ускорить отображение страниц для конечного пользователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завершение сессии Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
